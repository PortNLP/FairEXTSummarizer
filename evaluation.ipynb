{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Rouge Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import rouge\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs'\n",
    "output_directory = 'output_FairExtract/'\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "df = pd.DataFrame(columns=['Topic', 'R1', 'RL'])\n",
    "\n",
    "for file in file_list:\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-1', 'rouge-l'])\n",
    "\n",
    "    hypothesis = ' '. join(pd.read_csv(output_directory + file)['text'])\n",
    "    reference = ' '.join(pd.read_csv('input_docs/docs/' + file)['text'])\n",
    "\n",
    "    scores = evaluator.get_scores(hypothesis, reference)\n",
    "    new_row = pd.DataFrame({'Topic': [file[:-4]], 'R1': [scores[0]['rouge-1']['f']],\n",
    "                            'RL': [scores[0]['rouge-l']['f']]})\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "df = df.sort_values(by='Topic')\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.to_csv('evaluation/rouge_scores.csv', index=False)\n",
    "\n",
    "print(df.R1.mean(), df.RL.mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SummaQA Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from summaqa import QG_masked\n",
    "from summaqa import QA_Metric\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "question_generator = QG_masked()\n",
    "qa_metric = QA_Metric()\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs/'\n",
    "file_list = os.listdir(directory)\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    df = pd.DataFrame(columns=['Topic', 'SQA'])\n",
    "    for file in file_list:\n",
    "        docs = pd.read_csv(directory + file)\n",
    "        docs = docs[docs['label'] != remove_label]\n",
    "        article = ' '.join(docs['text'])\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "    \n",
    "        summary = ' '. join(pd.read_csv(output_directory + file)['text'])\n",
    "        score = qa_metric.compute(masked_questions, answer_spans, summary)\n",
    "    \n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], 'SQA': [score['avg_fscore']]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df = df.sort_values(by='Topic')\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/SummaQA-score.csv', index=False)\n",
    "    print(dir_dict[remove_label], df.SQA.mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BLANC Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from blanc import BlancHelp, BlancTune\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs/'\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    df = pd.DataFrame(columns=['Topic', 'BLANC-help', 'BLANC-tune'])\n",
    "    blanc_help = BlancHelp(device=device)\n",
    "    blanc_tune = BlancTune(device=device, finetune_mask_evenly=False, show_progress_bar=False)\n",
    "    \n",
    "    for file in file_list:\n",
    "        docs = pd.read_csv(directory + file)\n",
    "        docs = docs[docs['label'] != remove_label]\n",
    "        document = ' '.join(docs['text'])\n",
    "    \n",
    "        summary = ' '. join(pd.read_csv(output_directory + file)['text'])\n",
    "        score_h = blanc_help.eval_once(document, summary)\n",
    "        score_t = blanc_tune.eval_once(document, summary)\n",
    "    \n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], 'BLANC-help': [score_h],\n",
    "                                'BLANC-tune': [score_t]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df = df.sort_values(by='Topic')\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/BLANC-score.csv', index=False)\n",
    "    print(dir_dict[remove_label], df['BLANC-help'].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BARTScore Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/neulab/BARTScore"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from BARTScore.bart_score import BARTScorer\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs/'\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    df = pd.DataFrame(columns=['Topic', 'BARTScore'])\n",
    "    bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "    for file in file_list:\n",
    "        docs = pd.read_csv(directory + file)\n",
    "        docs = docs[docs['label'] != remove_label]\n",
    "        document = ' '.join(docs['text'])\n",
    "        \n",
    "        summary = ' '. join(pd.read_csv(output_directory + file)['text'])\n",
    "        score = bart_scorer.score([document], [summary], batch_size=4)\n",
    "    \n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], 'BARTScore': [score[0]]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='Topic')\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/BARTScore.csv', index=False)\n",
    "    print(dir_dict[remove_label], df['BARTScore'].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SUPERT Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/yg211/acl20-ref-free-eval.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"acl20-ref-free-eval\")\n",
    "\n",
    "from ref_free_metrics.supert import Supert\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs/'\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    df = pd.DataFrame(columns=['Topic', 'SUPERT'])\n",
    "    \n",
    "    for file in file_list:\n",
    "        docs = pd.read_csv(directory + file)\n",
    "        docs = docs[docs['label'] != remove_label]\n",
    "        document = [docs['text']]\n",
    "        \n",
    "        summary = [' '. join(pd.read_csv(output_directory + file)['text'])]\n",
    "\n",
    "        # compute the Supert scores\n",
    "        supert = Supert(document, ref_metric='top15')\n",
    "        score = supert(summary)\n",
    "    \n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], 'SUPERT': [score[0]]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df = df.sort_values(by='Topic')\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/SUPERT.csv', index=False)\n",
    "    print(dir_dict[remove_label], df['SUPERT'].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representation Gap Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the list of files in the directory\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    file_list = os.listdir(output_directory)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['Topic', 'Representation Gap'])\n",
    "    for file in file_list:\n",
    "        labels = pd.read_csv(output_directory + '/' + file)['label']\n",
    "        max_percentage = labels.value_counts().max() / labels.value_counts().sum()\n",
    "        min_percentage = 1 - max_percentage\n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], 'Representation Gap': [max_percentage - min_percentage]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "    df = df.sort_values(by='Topic')\n",
    "\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/Representation-Gap.csv', index=False)\n",
    "    print(dir_dict[remove_label], df['Representation Gap'].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UniEval Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the list of files in the directory\n",
    "directory = 'input_docs/docs/'\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "\n",
    "output_dir = 'outputs/output_FairGPT'\n",
    "eval_dir = 'FairGPT'\n",
    "\n",
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "evaluator = get_evaluator(\"summarization\")\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    output_directory = f'{output_dir}/{dir_dict[remove_label]}/'\n",
    "    df = pd.DataFrame(columns=['Topic', 'coherence', 'consistency', 'fluency', 'overall'])\n",
    "\n",
    "    for file in file_list:\n",
    "        docs = pd.read_csv(directory + file)\n",
    "        docs = docs[docs['label'] != remove_label]\n",
    "        document = ' '.join(docs['text'])\n",
    "\n",
    "        summary = [' '. join(pd.read_csv(output_directory + file)['text'])]\n",
    "\n",
    "        data = convert_to_json(output_list=summary,\n",
    "                               src_list=document)\n",
    "\n",
    "        # compute the UniEval scores\n",
    "        eval_scores = evaluator.evaluate(data, dims=['coherence', 'consistency', 'fluency'],\n",
    "                                         overall=True, print_result=True)\n",
    "        new_row = pd.DataFrame({'Topic': [file[:-4]], \n",
    "                                'coherence': eval_scores[0]['coherence'],\n",
    "                                'consistency' : eval_scores[0]['consistency'],\n",
    "                                'fluency': eval_scores[0]['fluency'],\n",
    "                                'overall': eval_scores[0]['overall']})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        clear_output()\n",
    "    df = df.sort_values(by='Topic')\n",
    "    df.to_csv(f'evaluation/{eval_dir}/{dir_dict[remove_label]}/UniEval.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All Metrics in same table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## in following we will put all csv files in one table\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the list of files in the directory\n",
    "group_list = ['H-A', 'A-W', 'W-H']\n",
    "\n",
    "directory_path = 'evaluation/'\n",
    "items = os.listdir(directory_path)\n",
    "\n",
    "# Filter out only the folders (directories)\n",
    "folders = [item for item in items if os.path.isdir(os.path.join(directory_path, item))]\n",
    "with pd.ExcelWriter('evaluation/eval.xlsx') as writer:\n",
    "    for folder in folders:\n",
    "        if folder in ['naive', 'naiveFair']:\n",
    "            df_result = pd.DataFrame(columns=['Topic', 'Groups', 'Iteration'])\n",
    "            for group in group_list:\n",
    "                for i in range(1, 6):\n",
    "                    directory = f'evaluation/{folder}/{group}/{i}/'\n",
    "                    file_list = os.listdir(directory)\n",
    "                    df = pd.DataFrame(columns=['Topic', 'Groups', 'Iteration'])\n",
    "                    for file in file_list:\n",
    "                        if file[-4:] == '.csv':\n",
    "                            new_df = pd.read_csv(directory + file)\n",
    "                            df = pd.merge(df, new_df, on='Topic', how='outer')\n",
    "                            df.Groups = df.Groups.fillna(group)\n",
    "                            df.Iteration = df.Iteration.fillna(i)\n",
    "                    df_result = pd.concat([df_result, df], ignore_index=True)\n",
    "        else:\n",
    "            df_result = pd.DataFrame(columns=['Topic', 'Groups'])\n",
    "            for group in group_list:\n",
    "                directory = f'evaluation/{folder}/{group}/'\n",
    "                file_list = os.listdir(directory)\n",
    "                df = pd.DataFrame(columns=['Topic', 'Groups'])    \n",
    "                for file in file_list:\n",
    "                    if file[-4:] == '.csv':\n",
    "                        new_df = pd.read_csv(directory + file)\n",
    "                        df = pd.merge(df, new_df, on='Topic', how='outer')\n",
    "                        df.Groups = df.Groups.fillna(group)\n",
    "                df_result = pd.concat([df_result, df], ignore_index=True)\n",
    "        df_result.to_excel(writer, sheet_name=folder, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
