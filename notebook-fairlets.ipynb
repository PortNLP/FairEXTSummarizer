{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from input CSV file\n",
      "Invalid color label in line 0 , skipping\n",
      "Number of data points: 60\n",
      "Dimension: 768\n",
      "Balance: 1 1\n",
      "Constructing tree...\n",
      "Doing fair clustering...\n",
      "Fairlet decomposition cost: 204.25336095191972\n",
      "Doing k-median clustering on fairlet centers...\n",
      "Computing fair k-median cost...\n",
      "Fairlet decomposition cost: 204.25336095191972\n",
      "k-Median cost: 312.27324521212574\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from failets import *\n",
    "\n",
    "### PARAMETERS ###\n",
    "topic = 'Xbox'\n",
    "\n",
    "blues = 1\n",
    "reds = 1\n",
    "k = 3\n",
    "input_csv_filename = \"input_docs/embedding/\"+ topic + \".csv\"\n",
    "remove_label = \"Hisp\"\n",
    "\n",
    "### MAIN ###\n",
    "label_dict = {'White': 0, 'Hisp': 1, 'AA': 2}\n",
    "df_temp = pd.read_csv(input_csv_filename)\n",
    "df_temp = df_temp[df_temp['label'] != label_dict[remove_label]]\n",
    "df_temp = df_temp.reset_index(drop=True)\n",
    "if remove_label == \"White\":\n",
    "    df_temp.replace({'label': {1: 0, 2: 1}}, inplace=True)\n",
    "elif remove_label == \"Hisp\":\n",
    "    df_temp.replace({'label': {0: 0, 2: 1}}, inplace=True)\n",
    "elif remove_label == \"AA\":\n",
    "    df_temp.replace({'label': {0: 0, 1: 1}}, inplace=True)\n",
    "else:\n",
    "    print(\"Invalid label_dict\")\n",
    "\n",
    "\n",
    "temp_address = 'df_temp.csv'\n",
    "df_temp.to_csv(temp_address, index=False)\n",
    "try:\n",
    "    p = min(blues, reds)\n",
    "    q = max(blues, reds)\n",
    "except:\n",
    "    print(\"First two parameters must be non-negative integers that specify the target balance; terminating\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# Parse input file in CSV format, first column is colors, other columns are coordinates\n",
    "print(\"Loading data from input CSV file\")\n",
    "colors = []\n",
    "points = []\n",
    "i = 0\n",
    "skipped_lines = 0\n",
    "for line in open(temp_address).readlines():\n",
    "    if len(line.strip()) == 0:\n",
    "        skipped_lines += 1\n",
    "        continue\n",
    "    tokens = line[:-1].split(\",\")\n",
    "    try:\n",
    "        color = int(tokens[0])\n",
    "    except:\n",
    "        print(\"Invalid color label in line\", i, \", skipping\")\n",
    "        skipped_lines += 1\n",
    "        continue\n",
    "    try:\n",
    "        point = [float(x) for x in tokens[1:]]\n",
    "    except:\n",
    "        print(\"Invalid point coordinates in line\", i, \", skipping\")\n",
    "        skipped_lines += 1\n",
    "        continue\n",
    "    colors.append(color)\n",
    "    points.append(point)\n",
    "    i += 1\n",
    "\n",
    "n_points = len(points)\n",
    "if  n_points == 0:\n",
    "    print(\"No successfully parsed points in input file, terminating\")\n",
    "    sys.exit(0)\n",
    "dimension = len(points[0])\n",
    "\n",
    "dataset = np.zeros((n_points, dimension))\n",
    "for i in range(n_points):\n",
    "    if len(points[i]) < dimension:\n",
    "        print(\"Insufficient dimension in line\", i+skipped_lines, \", terminating\")\n",
    "        sys.exit(0)\n",
    "    for j in range(dimension):\n",
    "        dataset[i,j] = points[i][j]\n",
    "\n",
    "print(\"Number of data points:\", n_points)\n",
    "print(\"Dimension:\", dimension)\n",
    "print(\"Balance:\", p, q)\n",
    "\n",
    "print(\"Constructing tree...\")\n",
    "fairlet_s = time.time()\n",
    "root = build_quadtree(dataset)\n",
    "\n",
    "print(\"Doing fair clustering...\")\n",
    "cost = tree_fairlet_decomposition(p, q, root, dataset, colors)\n",
    "fairlet_e = time.time()\n",
    "\n",
    "print(\"Fairlet decomposition cost:\", cost)\n",
    "\n",
    "print(\"Doing k-median clustering on fairlet centers...\")\n",
    "fairlet_center_idx = [dataset[index] for index in FAIRLET_CENTERS]\n",
    "fairlet_center_pt = np.array([np.array(xi) for xi in fairlet_center_idx])\n",
    "\n",
    "# Run k-medoids clustering\n",
    "cluster_s = time.time()\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=k, metric='euclidean')\n",
    "kmedoids.fit(fairlet_center_pt)\n",
    "\n",
    "# Get the cluster medoid locations\n",
    "C = kmedoids.cluster_centers_\n",
    "midx = kmedoids.medoid_indices_\n",
    "\n",
    "# Convert medoid indices to a column vector\n",
    "midx = np.array(midx).reshape(-1, 1)\n",
    "\n",
    "# Calculate the sum of distances for each sample to its closest medoid\n",
    "distances = kmedoids.transform(fairlet_center_pt)\n",
    "sumd = np.min(distances, axis=1)\n",
    "\n",
    "cluster_e = time.time()\n",
    "#np_idx = (np.array(idx._data)).flatten()\n",
    "\n",
    "# compute the indices of centers returned by Matlab in its input matrix\n",
    "# which is mat_matrix or fairlet_center_pt\n",
    "np_midx = (np.array(midx)).flatten()\n",
    "c_idx_matrix = np_midx.astype(int)\n",
    "#in matlab, arrays are numbered from 1\n",
    "c_idx_matrix[:] = [index - 1 for index in c_idx_matrix]\n",
    "\n",
    "# indices of center points in dataset\n",
    "centroids = [FAIRLET_CENTERS[index] for index in c_idx_matrix]\n",
    "\n",
    "print(\"Computing fair k-median cost...\")\n",
    "kmedian_cost = fair_kmedian_cost(centroids, dataset)\n",
    "print(\"Fairlet decomposition cost:\", cost)\n",
    "print(\"k-Median cost:\", kmedian_cost)\n",
    "\n",
    "def find_fairlet_medoids(lst, list_of_lists):\n",
    "    med_fairlet = []\n",
    "    for sublist in list_of_lists:\n",
    "        if any(element in sublist for element in lst):\n",
    "            med_fairlet.append(sublist)\n",
    "    return med_fairlet\n",
    "\n",
    "med_fairlet = find_fairlet_medoids(centroids, FAIRLETS)\n",
    "\n",
    "df_docs = pd.read_csv(\"input_docs/docs/\" + topic + \".csv\")\n",
    "df_docs = df_docs[df_docs['label'] != remove_label]\n",
    "df_docs = df_docs.reset_index(drop=True)\n",
    "\n",
    "df_output = pd.DataFrame(columns=['label', 'text', 'cluster'])\n",
    "for i in range(len(med_fairlet)):\n",
    "    cluster_data = df_docs.loc[med_fairlet[i]]\n",
    "    cluster_data['cluster'] = i + 1\n",
    "    df_output = pd.concat([df_output, cluster_data])\n",
    "\n",
    "df_output.to_csv(\"output/\" + topic + \".csv\", index=False)\n",
    "os.remove(temp_address)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T18:50:18.980578055Z",
     "start_time": "2023-09-15T18:50:18.468402803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
