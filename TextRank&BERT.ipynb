{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "lSGu2aSCpkxf"
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import random\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "directory_path = '/home/sina/Codes/fair-summarization/input_docs/docs'\n",
    "directory_files = os.listdir(directory_path)"
   ],
   "metadata": {
    "id": "PJlwPZ90V6JP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank Cluster-H\n"
   ],
   "metadata": {
    "id": "DwTnnc-TVy5-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_summary_df = pd.DataFrame(columns=['Topic','summary'])\n",
    "final_summary_df['Topic'] = list(directory_files)"
   ],
   "metadata": {
    "id": "vCCHBQud0FAN"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def combine_and_jumble_lists(list1, list2):\n",
    "  combined_list = list1 + list2\n",
    "  random.shuffle(combined_list)\n",
    "  return combined_list"
   ],
   "metadata": {
    "id": "GFicunf0cA6e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "  summary_list = []\n",
    "  for i in directory_files:\n",
    "    file = pd.read_csv(os.path.join(directory_path, i))\n",
    "    file = file[file['label'] != remove_label]\n",
    "    index = 0\n",
    "\n",
    "    grouped_list = list(file.groupby('label')['text'].apply(list).values)\n",
    "    list1 = grouped_list[0]\n",
    "    list2 = grouped_list[1]\n",
    "\n",
    "    sentences = list1\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "\n",
    "    summary_1 = []\n",
    "\n",
    "    for j in top_sentence_indices:\n",
    "      summary_1.append(sentences[j])\n",
    "\n",
    "    sentences = list2\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "    summary_2 = []\n",
    "\n",
    "    for k in top_sentence_indices:\n",
    "      summary_2.append(sentences[k])\n",
    "\n",
    "\n",
    "    jumbled_list = combine_and_jumble_lists(summary_1,summary_2)\n",
    "\n",
    "    sentences = jumbled_list\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "    summary_final = []\n",
    "\n",
    "    for l in top_sentence_indices:\n",
    "      summary_final.append(sentences[l])\n",
    "\n",
    "    summary_list.append(summary_final)\n",
    "  final_summary_df['summary'] = summary_list\n",
    "\n",
    "  for file in directory_files:\n",
    "    file_df = pd.read_csv(directory_path + '/' + file)\n",
    "    indices = []\n",
    "    for i in range(len(file_df)):\n",
    "      if file_df['text'][i] in final_summary_df['summary'].where(final_summary_df['Topic'] == file).dropna().values[0]:\n",
    "        indices.append(i)\n",
    "    file_df.iloc[indices].to_csv(f'output_TextRank_H/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "id": "PQc2d66Lan3K"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank Cluster A"
   ],
   "metadata": {
    "id": "G9zOn6kzc-iS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_summary_df = pd.DataFrame(columns=['Topic','summary'])\n",
    "final_summary_df['Topic'] = list(directory_files)"
   ],
   "metadata": {
    "id": "QweHqTnuwYaG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "  summary_list = []\n",
    "  for i in directory_files:\n",
    "    file = pd.read_csv(os.path.join(directory_path, i))\n",
    "    file = file[file['label'] != remove_label]\n",
    "    index = 0\n",
    "  \n",
    "    text_data = list(file['text'])\n",
    "    labels = list(file['label'])\n",
    "  \n",
    "  \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "  \n",
    "    label_encoder = LabelEncoder()\n",
    "    numeric_labels = label_encoder.fit_transform(labels)\n",
    "  \n",
    "    num_clusters = 2\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(X)\n",
    "  \n",
    "    predicted_labels = kmeans.labels_\n",
    "  \n",
    "    grouped_data = {}\n",
    "    for i, text in enumerate(text_data):\n",
    "      cluster_label = predicted_labels[i]\n",
    "      if cluster_label not in grouped_data:\n",
    "        grouped_data[cluster_label] = []\n",
    "      grouped_data[cluster_label].append(text)\n",
    "  \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "  \n",
    "    for key, value in grouped_data.items():\n",
    "      if key == 0:\n",
    "        cluster_0.extend(value)\n",
    "      elif key == 1:\n",
    "        cluster_1.extend(value)\n",
    "  \n",
    "    sentences = cluster_0\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "  \n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "  \n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "  \n",
    "    scores = nx.pagerank(graph)\n",
    "  \n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "  \n",
    "    summary_1 = []\n",
    "  \n",
    "    for j in top_sentence_indices:\n",
    "      summary_1.append(sentences[j])\n",
    "  \n",
    "  \n",
    "    sentences = cluster_1\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "  \n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "  \n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "  \n",
    "    scores = nx.pagerank(graph)\n",
    "  \n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "  \n",
    "    summary_2 = []\n",
    "  \n",
    "    for j in top_sentence_indices:\n",
    "      summary_2.append(sentences[j])\n",
    "  \n",
    "  \n",
    "    summary_final = summary_1 + summary_2\n",
    "  \n",
    "    sentences = summary_final\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "  \n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "  \n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "  \n",
    "    scores = nx.pagerank(graph)\n",
    "  \n",
    "    num_sentences = 6\n",
    "    top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "  \n",
    "    final = []\n",
    "  \n",
    "    for j in top_sentence_indices:\n",
    "      final.append(sentences[j])\n",
    "  \n",
    "  \n",
    "    summary_list.append(final)\n",
    "  final_summary_df['summary'] = summary_list\n",
    "\n",
    "  for file in directory_files:\n",
    "    file_df = pd.read_csv(directory_path + '/' + file)\n",
    "    indices = []\n",
    "    for i in range(len(file_df)):\n",
    "      if file_df['text'][i] in final_summary_df['summary'].where(final_summary_df['Topic'] == file).dropna().values[0]:\n",
    "        indices.append(i)\n",
    "    file_df.iloc[indices].to_csv(f'output_TextRank_A/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSWlwQ9rw2NY",
    "outputId": "502660cf-a81d-4b64-8185-54fddcbfbcf6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank Vanilla"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    summary_list = []\n",
    "    for i in directory_files:\n",
    "        file = pd.read_csv(os.path.join(directory_path, i))\n",
    "        file = file[file['label'] != remove_label]\n",
    "        \n",
    "        sentences = list(file['text'])\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "        similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "        scores = nx.pagerank(graph)\n",
    "\n",
    "        num_sentences = 6\n",
    "        top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n",
    "\n",
    "        summary_final = []\n",
    "        for j in top_sentence_indices:\n",
    "          summary_final.append(sentences[j])\n",
    "\n",
    "        summary_list.append(summary_final)\n",
    "    final_summary_df['summary'] = summary_list\n",
    "    \n",
    "\n",
    "    for file in directory_files:\n",
    "        file_df = pd.read_csv(directory_path + '/' + file)\n",
    "        indices = []\n",
    "        for i in range(len(file_df)):\n",
    "            if file_df['text'][i] in final_summary_df['summary'].where(final_summary_df['Topic'] == file).dropna().values[0]:\n",
    "                indices.append(i)\n",
    "        file_df.iloc[indices].to_csv(f'output_TextRank_V/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Extractive Cluster H"
   ],
   "metadata": {
    "id": "lELe8jpAC0NN"
   }
  },
  {
   "metadata": {
    "id": "hBxU_YunBFN_"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from summarizer import Summarizer"
  },
  {
   "cell_type": "code",
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def contains_only_symbols(input_string):\n",
    "    # Define a regular expression pattern to match symbols\n",
    "    symbol_pattern = r'^[\\W_]+$'\n",
    "\n",
    "    # Use re.match to check if the entire string matches the pattern\n",
    "    if re.match(symbol_pattern, input_string):\n",
    "        return True\n",
    "    elif input_string == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def longestSubstring(str1,str2):\n",
    "    seqMatch = SequenceMatcher(None,str1,str2)\n",
    "    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))\n",
    "    if (match.size!=0):\n",
    "        return str1[match.a: match.a + match.size]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def find_longest_common_subsequence(str1, str_list):\n",
    "    indices = []\n",
    "    subs = []\n",
    "    loop = 0\n",
    "    while not contains_only_symbols(str1):\n",
    "      if loop > 20:\n",
    "        break\n",
    "      lcs = [\"\"]*len(str_list)\n",
    "      for i, s in enumerate(str_list):\n",
    "          lcs[i] = longestSubstring(str1, s)\n",
    "      idx = lcs.index(max(lcs, key=len))\n",
    "      if not contains_only_symbols(lcs[idx]):  \n",
    "        indices.append(idx)\n",
    "        subs.append(lcs[idx])\n",
    "      str1 = str1.replace(lcs[idx], '')\n",
    "      lcs[idx] = ''\n",
    "      loop += 1\n",
    "    return indices, subs"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "  summary_list = []\n",
    "  for i in directory_files:\n",
    "    file = pd.read_csv(os.path.join(directory_path, i))\n",
    "    file = file[file['label'] != remove_label]\n",
    "    index = 0\n",
    "  \n",
    "    grouped_list = list(file.groupby('label')['text'].apply(list).values)\n",
    "    hispanic_list = grouped_list[0]\n",
    "    white_list = grouped_list[1]\n",
    "  \n",
    "    sentences = '.\\n '.join(hispanic_list)\n",
    "    summary_1 = []\n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "    summary_1.append(result)\n",
    "  \n",
    "    sentences = '.\\n '.join(white_list)\n",
    "  \n",
    "    summary_2 = []\n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "    summary_2.append(result)\n",
    "  \n",
    "  \n",
    "    jumbled_list = combine_and_jumble_lists(summary_1,summary_2)\n",
    "  \n",
    "    sentences = '.\\n '.join(jumbled_list)\n",
    "  \n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "    summary_list.append(result)\n",
    "  final_summary_df['summary'] = summary_list\n",
    "  \n",
    "  for file in directory_files:\n",
    "    file_df = pd.read_csv(directory_path + '/' + file)\n",
    "    summary = final_summary_df.where(final_summary_df['Topic'] == file).dropna().summary.values[0]\n",
    "    indices, subs = find_longest_common_subsequence(summary, file_df['text'].values)\n",
    "\n",
    "    # df = file_df.iloc[indices].copy()\n",
    "    # df.text = subs\n",
    "    # df.to_csv(f'output_BERT_H/{dir_dict[remove_label]}/{file}', index = False)\n",
    "\n",
    "    persentage = []\n",
    "    for i in range(len(indices)):\n",
    "      persentage.append(len(subs[i])/len(file_df['text'][indices[i]]))\n",
    "    top_indices = sorted(range(len(persentage)), key=lambda i: persentage[i], reverse=True)[:6]\n",
    "    selected_values = [indices[i] for i in top_indices]\n",
    "    file_df.iloc[selected_values].to_csv(f'output_BERT_H/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCLzUhkI6BTT",
    "outputId": "d378bc16-d1c9-42e6-d6eb-affc626c9625"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Extractive Cluster A"
   ],
   "metadata": {
    "id": "n-DjCWaoKJUu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "  summary_list = []\n",
    "  for i in directory_files:\n",
    "    summary_final = []\n",
    "    file = pd.read_csv(os.path.join(directory_path, i))\n",
    "    file = file[file['label'] != remove_label]\n",
    "    index = 0\n",
    "  \n",
    "    text_data = list(file['text'])\n",
    "    labels = list(file['label'])\n",
    "  \n",
    "  \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "  \n",
    "    label_encoder = LabelEncoder()\n",
    "    numeric_labels = label_encoder.fit_transform(labels)\n",
    "  \n",
    "    num_clusters = 2\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(X)\n",
    "  \n",
    "    predicted_labels = kmeans.labels_\n",
    "  \n",
    "    grouped_data = {}\n",
    "    for i, text in enumerate(text_data):\n",
    "      cluster_label = predicted_labels[i]\n",
    "      if cluster_label not in grouped_data:\n",
    "        grouped_data[cluster_label] = []\n",
    "      grouped_data[cluster_label].append(text)\n",
    "  \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "  \n",
    "    for key, value in grouped_data.items():\n",
    "      if key == 0:\n",
    "        cluster_0.extend(value)\n",
    "      elif key == 1:\n",
    "        cluster_1.extend(value)\n",
    "  \n",
    "    sentences = '. '.join(cluster_0)\n",
    "  \n",
    "    summary_1 = []\n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "    summary_1.append(result)\n",
    "  \n",
    "    sentences = '. '.join(cluster_1)\n",
    "  \n",
    "    summary_2 = []\n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "    summary_2.append(result)\n",
    "  \n",
    "  \n",
    "    summary_final = summary_1 + summary_2\n",
    "  \n",
    "    sentences = '. '.join(summary_final)\n",
    "  \n",
    "    model = Summarizer()\n",
    "    result = model(sentences, num_sentences=6)\n",
    "  \n",
    "    summary_list.append(result)\n",
    "  final_summary_df['summary'] = summary_list\n",
    "  \n",
    "  for file in directory_files:\n",
    "      file_df = pd.read_csv(directory_path + '/' + file)\n",
    "      summary = final_summary_df.where(final_summary_df['Topic'] == file).dropna().summary.values[0]\n",
    "      indices, subs = find_longest_common_subsequence(summary, file_df['text'].values)\n",
    "\n",
    "      # df = file_df.iloc[indices].copy()\n",
    "      # df.text = subs\n",
    "      # df.to_csv(f'output_BERT_H/{dir_dict[remove_label]}/{file}', index = False)\n",
    "\n",
    "      persentage = []\n",
    "      for i in range(len(indices)):\n",
    "          persentage.append(len(subs[i])/len(file_df['text'][indices[i]]))\n",
    "      top_indices = sorted(range(len(persentage)), key=lambda i: persentage[i], reverse=True)[:6]\n",
    "      selected_values = [indices[i] for i in top_indices]\n",
    "      file_df.iloc[selected_values].to_csv(f'output_BERT_A/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9E3R_E2HlYE",
    "outputId": "4c188b7f-1b76-462c-ec8a-f2a4b22a2f13"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Extractive Vanilla"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dir_dict = {'White': 'H-A', 'Hisp': 'A-W', 'AA': 'W-H'}\n",
    "\n",
    "for remove_label in ['White', 'Hisp', 'AA']:\n",
    "    summary_list = []\n",
    "    for i in directory_files:\n",
    "        file = pd.read_csv(os.path.join(directory_path, i))\n",
    "        file = file[file['label'] != remove_label]\n",
    "\n",
    "        sentences = '.\\n '.join(list(file['text']))\n",
    "\n",
    "        model = Summarizer()\n",
    "        result = model(sentences, num_sentences=6)\n",
    "        summary_list.append(result)\n",
    "    final_summary_df['summary'] = summary_list\n",
    "\n",
    "    for file in directory_files:\n",
    "        file_df = pd.read_csv(directory_path + '/' + file)\n",
    "        summary = final_summary_df.where(final_summary_df['Topic'] == file).dropna().summary.values[0]\n",
    "        indices, subs = find_longest_common_subsequence(summary, file_df['text'].values)\n",
    "    \n",
    "        # df = file_df.iloc[indices].copy()\n",
    "        # df.text = subs\n",
    "        # df.to_csv(f'output_BERT_H/{dir_dict[remove_label]}/{file}', index = False)\n",
    "    \n",
    "        persentage = []\n",
    "        for i in range(len(indices)):\n",
    "            persentage.append(len(subs[i])/len(file_df['text'][indices[i]]))\n",
    "        top_indices = sorted(range(len(persentage)), key=lambda i: persentage[i], reverse=True)[:6]\n",
    "        selected_values = [indices[i] for i in top_indices]\n",
    "        file_df.iloc[selected_values].to_csv(f'output_BERT_V/{dir_dict[remove_label]}/{file}', index = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
